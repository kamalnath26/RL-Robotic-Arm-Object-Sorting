{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "872e5850",
   "metadata": {},
   "source": [
    "# Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd578b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size, input_shape, n_actions):\n",
    "        self.size = max_size\n",
    "        self.counter = 0\n",
    "        self.states = np.zeros((max_size, input_shape))\n",
    "        self.actions = np.zeros((max_size, n_actions))\n",
    "        self.rewards = np.zeros(max_size)\n",
    "        self.new_states = np.zeros((max_size, input_shape))\n",
    "        self.terminal_state = np.zeros(max_size)\n",
    "    \n",
    "    def store_transition(self, state, action, reward, new_state, done):\n",
    "        index = self.counter % self.size\n",
    "\n",
    "        self.states[index] = state\n",
    "        self.actions[index] = action\n",
    "        self.rewards[index] = reward\n",
    "        self.new_states[index] = new_state\n",
    "        self.terminal_state[index] = done\n",
    "\n",
    "        self.counter += 1\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        max_memory = min(self.size, self.counter)\n",
    "        batch = np.random.choice(max_memory, batch_size, replace=False)\n",
    "\n",
    "        states = self.states[batch]\n",
    "        actions = self.actions[batch]\n",
    "        rewards = self.rewards[batch]\n",
    "        new_states = self.new_states[batch]\n",
    "        dones = self.terminal_state[batch]\n",
    "\n",
    "        return states, actions, rewards, new_states, dones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28105321",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa633ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "class ActorNetwork(keras.Model):\n",
    "    def __init__(self, n_actions, name, model, checkpoints_dir=\"ckp/\"):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        if not os.path.exists(checkpoints_dir):\n",
    "            os.mkdir(checkpoints_dir)\n",
    "        self.checkpoints_file = os.path.join(checkpoints_dir + model, name + \".h5\")\n",
    "\n",
    "        self.layer1 = Dense(512, activation=\"relu\")\n",
    "        self.layer2 = Dense(256, activation=\"relu\")\n",
    "        self.layer3 = Dense(256, activation=\"relu\")\n",
    "        self.pi = Dense(n_actions, activation=\"tanh\")\n",
    "    \n",
    "    @tf.function()\n",
    "    def call(self, state):\n",
    "        action = self.layer1(state)\n",
    "        action = self.layer2(action)\n",
    "        action = self.layer3(action)\n",
    "        pi = self.pi(action)\n",
    "\n",
    "        return pi\n",
    "\n",
    "class CriticNetwork(keras.Model):\n",
    "    def __init__(self, name, model, checkpoints_dir=\"ckp/\"):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        if not os.path.exists(checkpoints_dir):\n",
    "            os.mkdir(checkpoints_dir)\n",
    "        self.checkpoints_file = os.path.join(checkpoints_dir + model, name + \".h5\")\n",
    "\n",
    "        self.layer1 = Dense(512, activation=\"relu\")\n",
    "        self.layer2 = Dense(256, activation=\"relu\")\n",
    "        self.layer3 = Dense(256, activation=\"relu\")\n",
    "        self.q = Dense(1, activation=None)\n",
    "\n",
    "    @tf.function()\n",
    "    def call(self, state, action):\n",
    "        value = self.layer1(tf.concat([state, action], axis=1))\n",
    "        value = self.layer2(value)\n",
    "        value = self.layer3(value)\n",
    "        q = self.q(value)\n",
    "\n",
    "        return q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cf8a2d",
   "metadata": {},
   "source": [
    "# HER (Hindsight Experience Replay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21e46a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Perform HER memory augmentation\n",
    "def her_augmentation(agent, obs_array, actions, new_obs_array):\n",
    "    # Hyperparameter for future goals sampling\n",
    "    k = 4\n",
    "\n",
    "    # Augment the replay buffer\n",
    "    size = len(actions)\n",
    "    for index in range(size):\n",
    "        for _ in range(k):\n",
    "            future = np.random.randint(index, size)\n",
    "            _, future_achgoal, _ = new_obs_array[future].values()\n",
    "\n",
    "            obs, _, _ = obs_array[future].values()\n",
    "            state = np.concatenate((obs, future_achgoal, future_achgoal))\n",
    "\n",
    "            new_obs, _, _ = new_obs_array[future].values()\n",
    "            next_state = np.concatenate((new_obs, future_achgoal, future_achgoal))\n",
    "\n",
    "            action = actions[future]\n",
    "            reward = agent.env.compute_reward(future_achgoal, future_achgoal, 1.0)\n",
    "\n",
    "            # Store augmented experience in buffer\n",
    "            agent.remember(state, action, reward, next_state, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25488d4e",
   "metadata": {},
   "source": [
    "# DDPG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5941324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "# from replay_memory.ReplayBuffer import ReplayBuffer\n",
    "# from utils.networks import ActorNetwork, CriticNetwork\n",
    "\n",
    "## Actor-critic networks parameters :\n",
    "\n",
    "# actor learning rate\n",
    "alpha = 0.001\n",
    "\n",
    "# critic learning rate\n",
    "beta = 0.002\n",
    "\n",
    "## DDPG algorithms paramters\n",
    "\n",
    "# discount factor\n",
    "gamma = 0.99\n",
    "\n",
    "# target netwroks soft update factor \n",
    "tau = 0.005\n",
    "\n",
    "# replay buffer max memory size\n",
    "max_size = 10**6\n",
    "\n",
    "# exploration noise factor \n",
    "noise_factor = 0.1\n",
    "\n",
    "# training batch size \n",
    "batch_size = 64\n",
    "\n",
    "## DDPG agent class \n",
    "class DDPGAgent:\n",
    "    def __init__(self, env, input_dims):\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "        self.noise_factor = noise_factor\n",
    "\n",
    "        self.env = env\n",
    "        self.n_actions = env.action_space.shape[0]\n",
    "        self.max_action = env.action_space.high[0]\n",
    "        self.min_action = env.action_space.low[0]\n",
    "\n",
    "        self.memory = ReplayBuffer(max_size, input_dims, self.n_actions)\n",
    "\n",
    "        self._initialize_networks(self.n_actions)\n",
    "        self.update_parameters(tau=1)\n",
    "\n",
    "    # Choose action based on actor network\n",
    "    # Add exploration noise if in traning mode\n",
    "    def choose_action(self, state, evaluate=False):\n",
    "        state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
    "        actions = self.actor(state)\n",
    "        if not evaluate:\n",
    "            actions += tf.random.normal(shape=[self.n_actions], mean=0, stddev=self.noise_factor)\n",
    "        actions = tf.clip_by_value(actions, self.min_action, self.max_action)\n",
    "        return actions[0]\n",
    "    \n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        self.memory.store_transition(state, action, reward, new_state, done)\n",
    "    \n",
    "    # Main DDPG algorithms learning process\n",
    "    def learn(self):\n",
    "          if self.memory.counter < self.batch_size:\n",
    "              return\n",
    "\n",
    "          # Sample batch size of experiences from replay buffer\n",
    "          states, actions, rewards, new_states, dones = self.memory.sample(self.batch_size)\n",
    "          states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "          actions = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
    "          rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "          new_states = tf.convert_to_tensor(new_states, dtype=tf.float32)\n",
    "\n",
    "          # Calculate critic network loss\n",
    "          with tf.GradientTape() as tape:\n",
    "              target_actions = self.target_actor(new_states)\n",
    "              new_critic_value = tf.squeeze(self.target_critic(new_states, target_actions), 1)\n",
    "              critic_value = tf.squeeze(self.critic(states, actions), 1)\n",
    "              target = rewards + self.gamma * new_critic_value * (1 - dones)\n",
    "              critic_loss = tf.keras.losses.MSE(target, critic_value)\n",
    "\n",
    "          # Apply gradient decente with the calculated critic loss\n",
    "          critic_network_gradient = tape.gradient(critic_loss, self.critic.trainable_variables)\n",
    "          self.critic.optimizer.apply_gradients(zip(\n",
    "              critic_network_gradient, self.critic.trainable_variables \n",
    "          ))\n",
    "\n",
    "          # Calculate actor network loss\n",
    "          with tf.GradientTape() as tape:\n",
    "              new_actions = self.actor(states)\n",
    "              actor_loss = - self.critic(states, new_actions)\n",
    "              actor_loss = tf.math.reduce_mean(actor_loss)\n",
    "          \n",
    "          # Apply gradient decente with the calculated actor loss\n",
    "          actor_network_gradient = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "          self.actor.optimizer.apply_gradients(zip(\n",
    "                  actor_network_gradient, self.actor.trainable_variables \n",
    "              ))\n",
    "          \n",
    "          # Update actor/critic target networks\n",
    "          self.update_parameters()\n",
    "\n",
    "    # Update actor/critic target networks parameters with soft update rule\n",
    "    def update_parameters(self, tau=None):\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "        \n",
    "        weights = []\n",
    "        targets = self.target_actor.weights\n",
    "        for i, weight in enumerate(self.actor.weights):\n",
    "            weights.append(tau * weight + (1 - tau) * targets[i])\n",
    "        self.target_actor.set_weights(weights)\n",
    "\n",
    "        weights = []\n",
    "        targets = self.target_critic.weights\n",
    "        for i, weight in enumerate(self.critic.weights):\n",
    "            weights.append(tau * weight + (1 - tau) * targets[i])\n",
    "        self.target_critic.set_weights(weights)\n",
    "\n",
    "    def save_models(self):\n",
    "        print(\"---- saving models ----\")\n",
    "        self.actor.save_weights(self.actor.checkpoints_file)\n",
    "        self.critic.save_weights(self.critic.checkpoints_file)\n",
    "        self.target_actor.save_weights(self.target_actor.checkpoints_file)\n",
    "        self.target_critic.save_weights(self.target_critic.checkpoints_file)\n",
    "\n",
    "    def load_models(self):\n",
    "        print(\"---- loading models ----\")\n",
    "        self.actor.load_weights(self.actor.checkpoints_file)\n",
    "        self.critic.load_weights(self.critic.checkpoints_file)\n",
    "        self.target_actor.load_weights(self.target_actor.checkpoints_file)\n",
    "        self.target_critic.load_weights(self.target_critic.checkpoints_file)\n",
    "\n",
    "    def _initialize_networks(self, n_actions):\n",
    "        model = \"ddpg\"\n",
    "        self.actor = ActorNetwork(n_actions, name=\"actor\", model=model)\n",
    "        self.critic = CriticNetwork(name=\"critic\", model=model)\n",
    "        self.target_actor = ActorNetwork(n_actions, name=\"target_actor\", model=model)\n",
    "        self.target_critic = CriticNetwork(name=\"target_critic\", model=model)\n",
    "\n",
    "        self.actor.compile(keras.optimizers.Adam(learning_rate=alpha))\n",
    "        self.critic.compile(keras.optimizers.Adam(learning_rate=beta))\n",
    "        self.target_actor.compile(keras.optimizers.Adam(learning_rate=alpha))\n",
    "        self.target_critic.compile(keras.optimizers.Adam(learning_rate=beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efb4db0",
   "metadata": {},
   "source": [
    "# TD3 Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3bf464a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "# from replay_memory.ReplayBuffer import ReplayBuffer\n",
    "# from utils.networks import ActorNetwork, CriticNetwork\n",
    "\n",
    "## Actor-critic networks parameters :\n",
    "# actor learning rate\n",
    "alpha = 0.001\n",
    "\n",
    "# critic learning rate\n",
    "beta = 0.002\n",
    "\n",
    "## TD3 algorithms paramters\n",
    "# discount factor\n",
    "gamma = 0.99\n",
    "\n",
    "# target netwroks soft update factor\n",
    "tau = 0.05\n",
    "\n",
    "# replay buffer max memory size\n",
    "max_size = 10**6\n",
    "\n",
    "# exploration noise factor\n",
    "noise_factor = 0.1\n",
    "\n",
    "# training batch size\n",
    "batch_size = 256\n",
    "\n",
    "class TD3Agent:\n",
    "    def __init__(self, env, input_dims, update_actor_interval=2, warmup=500):\n",
    "        # setup hyperparameters values\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "        self.time_step = 0\n",
    "        self.warmup = warmup\n",
    "        self.learn_step_counter = 0\n",
    "        self.update_actor_interval = update_actor_interval\n",
    "        self.noise_factor = noise_factor\n",
    "\n",
    "        # setup environment\n",
    "        self.env = env\n",
    "        self.n_actions = env.action_space.shape[0]\n",
    "        self.max_action = env.action_space.high[0]\n",
    "        self.min_action = env.action_space.low[0]\n",
    "\n",
    "        # setup replay buffer memory\n",
    "        self.memory = ReplayBuffer(max_size, input_dims, self.n_actions)\n",
    "\n",
    "        # initialize actor and critic netwprks\n",
    "        self._initialize_networks(self.n_actions)\n",
    "        self.update_parameters(tau=1)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        \"\"\"\n",
    "        Choose an action for the agent.\n",
    "\n",
    "        If the time step is less than the warmup period, actions are selected randomly\n",
    "        from a normal distribution. This encourages exploration in the early stages,\n",
    "        with 'warmup' being a tunable hyperparameter.\n",
    "\n",
    "        After the time step is greater than or equal to the warmup period, actions are\n",
    "        determined using the actor network, with some noise added to the network's output\n",
    "        to promote exploration.\n",
    "\n",
    "        Args:\n",
    "            observation : The current observation/state of the environment.\n",
    "\n",
    "        Returns:\n",
    "            action: The chosen action.\n",
    "        \"\"\"\n",
    "        if self.time_step < self.warmup:\n",
    "            mu = np.random.normal(scale=self.noise_factor, size=(self.n_actions,))\n",
    "        else:\n",
    "            state = tf.convert_to_tensor([observation], dtype=tf.float32)\n",
    "            mu = self.actor(state)[0]\n",
    "        mu_star = mu + np.random.normal(scale=self.noise_factor)\n",
    "        mu_star = tf.clip_by_value(mu_star, self.min_action, self.max_action)\n",
    "        self.time_step += 1\n",
    "\n",
    "        return mu_star\n",
    "\n",
    "    def remember(self, state, action, reward, new_state, done):\n",
    "        \"\"\"\n",
    "        Interface function between agent and buffer, used to store transitions\n",
    "        \"\"\"\n",
    "        self.memory.store_transition(state, action, reward, new_state, done)\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"\n",
    "        Main agent learning function implementing the TD3 algorithm.\n",
    "\n",
    "        This function trains the agent using the following steps:\n",
    "        1. Sample a random batch of old experiences from memory.\n",
    "        2. Apply gradient descent on the two critic networks.\n",
    "        3. Apply gradient descent on the actor network in a delayed\n",
    "        manner: actor is updated once for every two updates of critic networks.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        # Check if there are enough experiences in memory to begin training\n",
    "        if self.memory.counter < self.batch_size:\n",
    "            return\n",
    "\n",
    "        # Step 1: Sample a random batch of old experiences from memory\n",
    "        states, actions, rewards, new_states, dones = self.memory.sample(self.batch_size)\n",
    "        states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
    "        rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "        new_states = tf.convert_to_tensor(new_states, dtype=tf.float32)\n",
    "\n",
    "        # Step 2: Apply gradient descent on the two critic networks\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # Calculate target actions with some noise\n",
    "            target_actions = self.target_actor(new_states)\n",
    "            target_actions += tf.clip_by_value(np.random.normal(scale=0.2), -0.5, 0.5)\n",
    "            target_actions = tf.clip_by_value(target_actions, self.min_action, self.max_action)\n",
    "\n",
    "            # Compute Q-values and target Q-values for both critic networks\n",
    "            q1 = tf.squeeze(self.critic_1(states, actions), 1)\n",
    "            q2 = tf.squeeze(self.critic_2(states, actions), 1)\n",
    "            q1_new = tf.squeeze(self.target_critic_1(new_states, target_actions), 1)\n",
    "            q2_new = tf.squeeze(self.target_critic_2(new_states, target_actions), 1)\n",
    "            target = rewards + self.gamma * tf.math.minimum(q1_new, q2_new) * (1 - dones)\n",
    "\n",
    "            # Compute critic losses\n",
    "            critic_1_loss = keras.losses.MSE(target, q1)\n",
    "            critic_2_loss = keras.losses.MSE(target, q2)\n",
    "\n",
    "        # Compute critic gradients and apply gradient descent\n",
    "        critic_1_gradient = tape.gradient(critic_1_loss, self.critic_1.trainable_variables)\n",
    "        critic_2_gradient = tape.gradient(critic_2_loss, self.critic_2.trainable_variables)\n",
    "        self.critic_1.optimizer.apply_gradients(zip(critic_1_gradient, self.critic_1.trainable_variables))\n",
    "        self.critic_2.optimizer.apply_gradients(zip(critic_2_gradient, self.critic_2.trainable_variables))\n",
    "\n",
    "        # Step 3: Update the actor network only once for every two updates of critic networks\n",
    "        self.learn_step_counter += 1\n",
    "        if self.learn_step_counter % self.update_actor_interval != 0:\n",
    "            return\n",
    "\n",
    "        # Apply gradient descent on the actor network\n",
    "        with tf.GradientTape() as tape:\n",
    "            new_actions = self.actor(states)\n",
    "            critic_1_value = self.critic_1(states, new_actions)\n",
    "            actor_loss = -tf.math.reduce_mean(critic_1_value)\n",
    "\n",
    "        # Compute actor gradients and apply gradient descent\n",
    "        actor_gardient = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "        self.actor.optimizer.apply_gradients(zip(actor_gardient, self.actor.trainable_variables))\n",
    "\n",
    "        # Update actor/critic target networks weights with soft update rule\n",
    "        self.update_parameters()\n",
    "\n",
    "    def update_parameters(self, tau=None):\n",
    "        \"\"\"\n",
    "        Update the weights of the target actor and both target critic networks using a soft update rule.\n",
    "\n",
    "        The formula used for the soft update is as follows:\n",
    "            new_weight = tau * old_weight + (1 - tau) * old_target_weight\n",
    "\n",
    "        Args:\n",
    "            tau (float, optional): The interpolation parameter for the soft update.\n",
    "                If not provided, the default tau value from the class attributes is used.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "\n",
    "        # Update the weights of the target actor\n",
    "        self._update_target_network(self.target_actor, self.actor, tau)\n",
    "\n",
    "        # Update the weights of the first target critic network\n",
    "        self._update_target_network(self.target_critic_1, self.critic_1, tau)\n",
    "\n",
    "        # Update the weights of the second target critic network\n",
    "        self._update_target_network(self.target_critic_2, self.critic_2, tau)\n",
    "\n",
    "    def save_models(self):\n",
    "        print(\"---- saving models ----\")\n",
    "        self.actor.save_weights(self.actor.checkpoints_file)\n",
    "        self.critic_1.save_weights(self.critic_1.checkpoints_file)\n",
    "        self.critic_2.save_weights(self.critic_2.checkpoints_file)\n",
    "        self.target_actor.save_weights(self.target_actor.checkpoints_file)\n",
    "        self.target_critic_1.save_weights(self.target_critic_1.checkpoints_file)\n",
    "        self.target_critic_2.save_weights(self.target_critic_2.checkpoints_file)\n",
    "\n",
    "    def load_models(self):\n",
    "        print(\"---- loading models ----\")\n",
    "        self.actor.load_weights(self.actor.checkpoints_file)\n",
    "        self.critic_1.load_weights(self.critic_1.checkpoints_file)\n",
    "        self.critic_2.load_weights(self.critic_2.checkpoints_file)\n",
    "        self.target_actor.load_weights(self.target_actor.checkpoints_file)\n",
    "        self.target_critic_1.load_weights(self.target_critic_1.checkpoints_file)\n",
    "        self.target_critic_2.load_weights(self.target_critic_2.checkpoints_file)\n",
    "\n",
    "    def _initialize_networks(self, n_actions):\n",
    "        model = \"TD3\"\n",
    "        self.actor = ActorNetwork(n_actions, name=\"actor\", model=model)\n",
    "        self.critic_1 = CriticNetwork(name=\"critic_1\", model=model)\n",
    "        self.critic_2 = CriticNetwork(name=\"critic_2\", model=model)\n",
    "\n",
    "        self.target_actor = ActorNetwork(n_actions, name=\"target_actor\", model=model)\n",
    "        self.target_critic_1 = CriticNetwork(name=\"target_critic_1\", model=model)\n",
    "        self.target_critic_2 = CriticNetwork(name=\"target_critic_2\", model=model)\n",
    "\n",
    "        self.actor.compile(keras.optimizers.Adam(learning_rate=alpha), loss=\"mean\")\n",
    "        self.critic_1.compile(keras.optimizers.Adam(learning_rate=beta), loss=\"mean_squared_error\")\n",
    "        self.critic_2.compile(keras.optimizers.Adam(learning_rate=beta), loss=\"mean_squared_error\")\n",
    "\n",
    "        self.target_actor.compile(keras.optimizers.Adam(learning_rate=alpha), loss=\"mean\")\n",
    "        self.target_critic_1.compile(keras.optimizers.Adam(learning_rate=beta), loss=\"mean_squared_error\")\n",
    "        self.target_critic_2.compile(keras.optimizers.Adam(learning_rate=beta), loss=\"mean_squared_error\")\n",
    "\n",
    "    def _update_target_network(self, target_network, source_network, tau):\n",
    "        \"\"\"\n",
    "        Update the weights of a target neural network using a soft update rule.\n",
    "\n",
    "        Args:\n",
    "            target_network (tf.keras.Model): The target neural network whose weights need to be updated.\n",
    "            source_network (tf.keras.Model): The source neural network from which weights are copied.\n",
    "            tau (float): The interpolation parameter for the soft update.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        weights = []\n",
    "        target_weights = target_network.weights\n",
    "        for i, weight in enumerate(source_network.weights):\n",
    "            weights.append(tau * weight + (1 - tau) * target_weights[i])\n",
    "        target_network.set_weights(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a9abaa",
   "metadata": {},
   "source": [
    "# DDPG Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "471e306f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Jan 29 2025 23:16:28\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m score_history \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     14\u001b[0m avg_score_history \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 16\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPandaReach-v3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m obs_shape \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mobservation_space[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobservation\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \\\n\u001b[1;32m     18\u001b[0m             env\u001b[38;5;241m.\u001b[39mobservation_space[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124machieved_goal\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \\\n\u001b[1;32m     19\u001b[0m             env\u001b[38;5;241m.\u001b[39mobservation_space[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdesired_goal\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     21\u001b[0m agent \u001b[38;5;241m=\u001b[39m DDPGAgent(env\u001b[38;5;241m=\u001b[39menv, input_dims\u001b[38;5;241m=\u001b[39mobs_shape)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/envs/registration.py:704\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m     env_creator \u001b[38;5;241m=\u001b[39m env_spec\u001b[38;5;241m.\u001b[39mentry_point\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# Assume it's a string\u001b[39;00m\n\u001b[0;32m--> 704\u001b[0m     env_creator \u001b[38;5;241m=\u001b[39m \u001b[43mload_env_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_spec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentry_point\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;66;03m# Determine if to use the rendering\u001b[39;00m\n\u001b[1;32m    707\u001b[0m render_modes: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/envs/registration.py:551\u001b[0m, in \u001b[0;36mload_env_creator\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Loads an environment with name of style ``\"(import path):(environment name)\"`` and returns the environment creation function, normally the environment class type.\u001b[39;00m\n\u001b[1;32m    543\u001b[0m \n\u001b[1;32m    544\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;124;03m    The environment constructor for the given environment name.\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    550\u001b[0m mod_name, attr_name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 551\u001b[0m mod \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    552\u001b[0m fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(mod, attr_name)\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn\n",
      "File \u001b[0;32m/usr/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/panda_gym/envs/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpanda_gym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpanda_tasks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     PandaFlipEnv,\n\u001b[1;32m      3\u001b[0m     PandaPickAndPlaceEnv,\n\u001b[1;32m      4\u001b[0m     PandaPushEnv,\n\u001b[1;32m      5\u001b[0m     PandaReachEnv,\n\u001b[1;32m      6\u001b[0m     PandaSlideEnv,\n\u001b[1;32m      7\u001b[0m     PandaStackEnv,\n\u001b[1;32m      8\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/panda_gym/envs/panda_tasks.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpanda_gym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RobotTaskEnv\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpanda_gym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrobots\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpanda\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Panda\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpanda_gym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflip\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Flip\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpanda_gym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpick_and_place\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PickAndPlace\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpanda_gym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpush\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Push\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/panda_gym/envs/tasks/flip.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, Tuple\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransform\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Rotation \u001b[38;5;28;01mas\u001b[39;00m R\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpanda_gym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Task\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpanda_gym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpybullet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PyBullet\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/scipy/spatial/__init__.py:102\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m=============================================================\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mSpatial algorithms and data structures (:mod:`scipy.spatial`)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m   QhullError\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_kdtree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ckdtree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_qhull\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/scipy/spatial/_kdtree.py:5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ckdtree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cKDTree, cKDTreeNode\n\u001b[1;32m      7\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminkowski_distance_p\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminkowski_distance\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistance_matrix\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      9\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRectangle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKDTree\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mminkowski_distance_p\u001b[39m(x, y, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m):\n",
      "File \u001b[0;32m_ckdtree.pyx:1\u001b[0m, in \u001b[0;36minit scipy.spatial._ckdtree\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import panda_gym\n",
    "# from agents.ddpg import DDPGAgent\n",
    "# from utils.HER import her_augmentation\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    n_games = 1500\n",
    "    opt_steps = 64\n",
    "    best_score = 0\n",
    "    score_history = []\n",
    "    avg_score_history = []\n",
    "    \n",
    "    env = gym.make('PandaReach-v3')\n",
    "    obs_shape = env.observation_space['observation'].shape[0] + \\\n",
    "                env.observation_space['achieved_goal'].shape[0] + \\\n",
    "                env.observation_space['desired_goal'].shape[0]\n",
    "\n",
    "    agent = DDPGAgent(env=env, input_dims=obs_shape)\n",
    "\n",
    "    for i in range(n_games):\n",
    "        done = False\n",
    "        truncated = False\n",
    "        score = 0\n",
    "        step = 0\n",
    "\n",
    "        obs_array = []\n",
    "        actions_array = []\n",
    "        new_obs_array = []\n",
    "\n",
    "        observation, info = env.reset()\n",
    "\n",
    "        while not (done or truncated):\n",
    "            curr_obs, curr_achgoal, curr_desgoal = observation.values()\n",
    "            state = np.concatenate((curr_obs, curr_achgoal, curr_desgoal))\n",
    "\n",
    "            # Choose an action\n",
    "            action = agent.choose_action(state, False)\n",
    "\n",
    "            # Excute the choosen action in the environement\n",
    "            new_observation, reward, done, truncated, _ = env.step(np.array(action))\n",
    "            next_obs, next_achgoal, next_desgoal = new_observation.values()\n",
    "            new_state = np.concatenate((next_obs, next_achgoal, next_desgoal))\n",
    "\n",
    "            # Store experience in the replay buffer\n",
    "            agent.remember(state, action, reward, new_state, done)\n",
    "        \n",
    "            obs_array.append(observation)\n",
    "            actions_array.append(action)\n",
    "            new_obs_array.append(new_observation)\n",
    "\n",
    "            observation = new_observation\n",
    "            score += reward\n",
    "            step += 1\n",
    "        \n",
    "        # Augmente replay buffer with HER\n",
    "        her_augmentation(agent, obs_array, actions_array, new_obs_array)\n",
    "\n",
    "        # train the agent in multiple optimization steps\n",
    "        for _ in range(opt_steps):\n",
    "          agent.learn()\n",
    "            \n",
    "        score_history.append(score)\n",
    "        avg_score = np.mean(score_history[-100:])\n",
    "        avg_score_history.append(avg_score)\n",
    "\n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "        \n",
    "        print(f\"Episode {i} steps {step} score {score:.1f} avg score {avg_score:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d07354",
   "metadata": {},
   "source": [
    "# TD3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55e08bd9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m score_history \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     13\u001b[0m avg_score_history \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 15\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPandaReach-v3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m obs_shape \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mobservation_space[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobservation\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \\\n\u001b[1;32m     17\u001b[0m             env\u001b[38;5;241m.\u001b[39mobservation_space[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124machieved_goal\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \\\n\u001b[1;32m     18\u001b[0m             env\u001b[38;5;241m.\u001b[39mobservation_space[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdesired_goal\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     20\u001b[0m agent \u001b[38;5;241m=\u001b[39m TD3Agent(env\u001b[38;5;241m=\u001b[39menv, input_dims\u001b[38;5;241m=\u001b[39mobs_shape)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/envs/registration.py:704\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m     env_creator \u001b[38;5;241m=\u001b[39m env_spec\u001b[38;5;241m.\u001b[39mentry_point\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# Assume it's a string\u001b[39;00m\n\u001b[0;32m--> 704\u001b[0m     env_creator \u001b[38;5;241m=\u001b[39m \u001b[43mload_env_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_spec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentry_point\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;66;03m# Determine if to use the rendering\u001b[39;00m\n\u001b[1;32m    707\u001b[0m render_modes: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/envs/registration.py:551\u001b[0m, in \u001b[0;36mload_env_creator\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Loads an environment with name of style ``\"(import path):(environment name)\"`` and returns the environment creation function, normally the environment class type.\u001b[39;00m\n\u001b[1;32m    543\u001b[0m \n\u001b[1;32m    544\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;124;03m    The environment constructor for the given environment name.\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    550\u001b[0m mod_name, attr_name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 551\u001b[0m mod \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    552\u001b[0m fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(mod, attr_name)\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn\n",
      "File \u001b[0;32m/usr/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/panda_gym/envs/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpanda_gym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpanda_tasks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     PandaFlipEnv,\n\u001b[1;32m      3\u001b[0m     PandaPickAndPlaceEnv,\n\u001b[1;32m      4\u001b[0m     PandaPushEnv,\n\u001b[1;32m      5\u001b[0m     PandaReachEnv,\n\u001b[1;32m      6\u001b[0m     PandaSlideEnv,\n\u001b[1;32m      7\u001b[0m     PandaStackEnv,\n\u001b[1;32m      8\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/panda_gym/envs/panda_tasks.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpanda_gym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RobotTaskEnv\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpanda_gym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrobots\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpanda\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Panda\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpanda_gym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflip\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Flip\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpanda_gym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpick_and_place\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PickAndPlace\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpanda_gym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpush\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Push\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/panda_gym/envs/tasks/flip.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, Tuple\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransform\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Rotation \u001b[38;5;28;01mas\u001b[39;00m R\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpanda_gym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Task\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpanda_gym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpybullet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PyBullet\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/scipy/spatial/__init__.py:102\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m=============================================================\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mSpatial algorithms and data structures (:mod:`scipy.spatial`)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m   QhullError\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_kdtree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ckdtree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_qhull\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/scipy/spatial/_kdtree.py:5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ckdtree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cKDTree, cKDTreeNode\n\u001b[1;32m      7\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminkowski_distance_p\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminkowski_distance\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistance_matrix\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      9\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRectangle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKDTree\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mminkowski_distance_p\u001b[39m(x, y, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m):\n",
      "File \u001b[0;32m_ckdtree.pyx:1\u001b[0m, in \u001b[0;36minit scipy.spatial._ckdtree\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import panda_gym\n",
    "# from agents.td3 import TD3Agent\n",
    "# from utils.HER import her_augmentation\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    n_games = 1500\n",
    "    opt_steps = 64\n",
    "    best_score = 0\n",
    "    score_history = []\n",
    "    avg_score_history = []\n",
    "    \n",
    "    env = gym.make('PandaReach-v3')\n",
    "    obs_shape = env.observation_space['observation'].shape[0] + \\\n",
    "                env.observation_space['achieved_goal'].shape[0] + \\\n",
    "                env.observation_space['desired_goal'].shape[0]\n",
    "\n",
    "    agent = TD3Agent(env=env, input_dims=obs_shape)\n",
    "\n",
    "    for i in range(n_games):\n",
    "        done = False\n",
    "        truncated = False\n",
    "        score = 0\n",
    "        step = 0\n",
    "\n",
    "        obs_array = []\n",
    "        actions_array = []\n",
    "        new_obs_array = []\n",
    "\n",
    "        observation, info = env.reset()\n",
    "\n",
    "        while not (done or truncated):\n",
    "            curr_obs, curr_achgoal, curr_desgoal = observation.values()\n",
    "            state = np.concatenate((curr_obs, curr_achgoal, curr_desgoal))\n",
    "\n",
    "            # Choose an action\n",
    "            action = agent.choose_action(state, False)\n",
    "\n",
    "            # Excute the choosen action in the environement\n",
    "            new_observation, reward, done, truncated, _ = env.step(np.array(action))\n",
    "            next_obs, next_achgoal, next_desgoal = new_observation.values()\n",
    "            new_state = np.concatenate((next_obs, next_achgoal, next_desgoal))\n",
    "\n",
    "            # Store experience in the replay buffer\n",
    "            agent.remember(state, action, reward, new_state, done)\n",
    "        \n",
    "            obs_array.append(observation)\n",
    "            actions_array.append(action)\n",
    "            new_obs_array.append(new_observation)\n",
    "\n",
    "            observation = new_observation\n",
    "            score += reward\n",
    "            step += 1\n",
    "        \n",
    "        # Augmente replay buffer with HER\n",
    "        her_augmentation(agent, obs_array, actions_array, new_obs_array)\n",
    "\n",
    "        # train the agent in multiple optimization steps\n",
    "        for _ in range(opt_steps):\n",
    "          agent.learn()\n",
    "            \n",
    "        score_history.append(score)\n",
    "        avg_score = np.mean(score_history[-100:])\n",
    "        avg_score_history.append(avg_score)\n",
    "\n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "        \n",
    "        print(f\"Episode {i} steps {step} score {score:.1f} avg score {avg_score:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914ac58c",
   "metadata": {},
   "source": [
    "# Training with save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81ed9961",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# from agents.ddpg import DDPGAgent\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPandaReach-v3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrgb_array\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m obs_shape \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mobservation_space[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobservation\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \\\n\u001b[1;32m      9\u001b[0m                 env\u001b[38;5;241m.\u001b[39mobservation_space[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124machieved_goal\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \\\n\u001b[1;32m     10\u001b[0m                 env\u001b[38;5;241m.\u001b[39mobservation_space[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdesired_goal\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Choose your trained agent : DDPG or TD3\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/envs/registration.py:704\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m     env_creator \u001b[38;5;241m=\u001b[39m env_spec\u001b[38;5;241m.\u001b[39mentry_point\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# Assume it's a string\u001b[39;00m\n\u001b[0;32m--> 704\u001b[0m     env_creator \u001b[38;5;241m=\u001b[39m \u001b[43mload_env_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_spec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentry_point\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;66;03m# Determine if to use the rendering\u001b[39;00m\n\u001b[1;32m    707\u001b[0m render_modes: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gymnasium/envs/registration.py:551\u001b[0m, in \u001b[0;36mload_env_creator\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Loads an environment with name of style ``\"(import path):(environment name)\"`` and returns the environment creation function, normally the environment class type.\u001b[39;00m\n\u001b[1;32m    543\u001b[0m \n\u001b[1;32m    544\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;124;03m    The environment constructor for the given environment name.\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    550\u001b[0m mod_name, attr_name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 551\u001b[0m mod \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    552\u001b[0m fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(mod, attr_name)\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn\n",
      "File \u001b[0;32m/usr/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/panda_gym/envs/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpanda_gym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpanda_tasks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     PandaFlipEnv,\n\u001b[1;32m      3\u001b[0m     PandaPickAndPlaceEnv,\n\u001b[1;32m      4\u001b[0m     PandaPushEnv,\n\u001b[1;32m      5\u001b[0m     PandaReachEnv,\n\u001b[1;32m      6\u001b[0m     PandaSlideEnv,\n\u001b[1;32m      7\u001b[0m     PandaStackEnv,\n\u001b[1;32m      8\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/panda_gym/envs/panda_tasks.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpanda_gym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RobotTaskEnv\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpanda_gym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrobots\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpanda\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Panda\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpanda_gym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflip\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Flip\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpanda_gym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpick_and_place\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PickAndPlace\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpanda_gym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtasks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpush\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Push\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/panda_gym/envs/tasks/flip.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, Tuple\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransform\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Rotation \u001b[38;5;28;01mas\u001b[39;00m R\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpanda_gym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Task\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpanda_gym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpybullet\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PyBullet\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/scipy/spatial/__init__.py:102\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m=============================================================\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mSpatial algorithms and data structures (:mod:`scipy.spatial`)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m   QhullError\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_kdtree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ckdtree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_qhull\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/scipy/spatial/_kdtree.py:5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ckdtree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cKDTree, cKDTreeNode\n\u001b[1;32m      7\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminkowski_distance_p\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminkowski_distance\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistance_matrix\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      9\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRectangle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKDTree\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mminkowski_distance_p\u001b[39m(x, y, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m):\n",
      "File \u001b[0;32m_ckdtree.pyx:1\u001b[0m, in \u001b[0;36minit scipy.spatial._ckdtree\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import panda_gym\n",
    "from numpngw import write_apng\n",
    "from IPython.display import Image\n",
    "# from agents.ddpg import DDPGAgent\n",
    "\n",
    "env = gym.make(\"PandaReach-v3\", render_mode=\"rgb_array\")\n",
    "obs_shape = env.observation_space['observation'].shape[0] + \\\n",
    "                env.observation_space['achieved_goal'].shape[0] + \\\n",
    "                env.observation_space['desired_goal'].shape[0]\n",
    "\n",
    "# Choose your trained agent : DDPG or TD3\n",
    "agent = DDPGAgent(env=env, input_dims=obs_shape)\n",
    "# load pre-trained networks weights\n",
    "agent.load_models()\n",
    "\n",
    "observation, info = env.reset()\n",
    "\n",
    "# Stores frames of robot arm moving in Reacher env\n",
    "images = [env.render()]\n",
    "\n",
    "done = False\n",
    "truncated = False\n",
    "for i in range(200):\n",
    "    curr_obs, curr_achgoal, curr_desgoal = observation.values()\n",
    "    state = np.concatenate((curr_obs, curr_achgoal, curr_desgoal))\n",
    "\n",
    "    # Choose an action using pre-trainded RL agent\n",
    "    action = agent.choose_action(state)\n",
    "\n",
    "    # Excute the choosen action in the environement\n",
    "    new_observation, reward, done, truncated, _ = env.step(np.array(action))\n",
    "    images.append(env.render())\n",
    "    observation = new_observation\n",
    "\n",
    "    if done or truncated:\n",
    "        observation, info = env.reset()\n",
    "        images.append(env.render())\n",
    "\n",
    "env.close()\n",
    "\n",
    "# save frames : real-time rendering = 40 ms between frames\n",
    "write_apng(\"anim.png\", images, delay=60)\n",
    "# show movements\n",
    "Image(filename=\"anim.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
